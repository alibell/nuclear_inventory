{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import string\n",
    "\n",
    "def get_file_list_from_dir(*, path, datadir):\n",
    "    data_files = sorted(glob(os.path.join(path, \"data\", datadir, \"*.csv.gz\")))\n",
    "    return data_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = list(string.ascii_uppercase)  # to ease the manipulation of the data\n",
    "input_compos = alphabet[:8]\n",
    "input_params = input_compos + [\"p1\", \"p2\", \"p3\", \"p4\", \"p5\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dirty preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = get_file_list_from_dir(path=\".\", datadir=\"train\")\n",
    "dtrain = pd.concat((pd.read_csv(f) for f in train_files))\n",
    "\n",
    "train_data = dtrain[alphabet].add_prefix('Y_')\n",
    "train_data[\"times\"] = dtrain[\"times\"]\n",
    "train_data = train_data[ train_data[\"times\"] > 0.]\n",
    "temp = dtrain.loc[0][input_params].reset_index(drop=True)\n",
    "temp = temp.loc[temp.index.repeat(80)].reset_index(drop=True)\n",
    "train_data = pd.concat([temp, train_data.reset_index(drop=True)], axis=1)\n",
    "\n",
    "train_target_A = train_data.groupby(input_params)['Y_A'].apply(list).apply(pd.Series).rename(\n",
    "    columns=lambda x: 'A' + str(x + 1)).reset_index()\n",
    "train_target_A\n",
    "\n",
    "X_train = train_target_A[input_params]\n",
    "\n",
    "y_train_all = []\n",
    "for i in alphabet:\n",
    "    y_train_all.append(\n",
    "        train_data.groupby(input_params)['Y_'+i].apply(list).apply(pd.Series).rename(\n",
    "        columns=lambda x: i + str(x + 1)).reset_index().iloc[:, len(input_params):]\n",
    "    )\n",
    "    \n",
    "y_train_all = pd.concat(y_train_all, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files = get_file_list_from_dir(path=\".\", datadir=\"test\")\n",
    "dtest = pd.concat((pd.read_csv(f) for f in test_files))\n",
    "\n",
    "test_data = dtest[alphabet].add_prefix('Y_')\n",
    "test_data[\"times\"] = dtest[\"times\"]\n",
    "test_data = test_data[test_data[\"times\"] > 0.]\n",
    "temp = dtest.loc[0][input_params].reset_index(drop=True)\n",
    "temp = temp.loc[temp.index.repeat(80)].reset_index(drop=True)\n",
    "test_data = pd.concat([temp, test_data.reset_index(drop=True)], axis=1)\n",
    "\n",
    "test_target_A = test_data.groupby(input_params)['Y_A'].apply(list).apply(pd.Series).rename(\n",
    "    columns=lambda x: 'A' + str(x + 1)).reset_index()\n",
    "\n",
    "X_test = test_target_A[input_params]\n",
    "\n",
    "y_test_all = []\n",
    "for i in alphabet:\n",
    "    y_test_all.append(\n",
    "        test_data.groupby(input_params)['Y_'+i].apply(list).apply(pd.Series).rename(\n",
    "        columns=lambda x: i + str(x + 1)).reset_index().iloc[:, len(input_params):]\n",
    "    )\n",
    "    \n",
    "y_test_all = pd.concat(y_test_all, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Current work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from functools import reduce\n",
    "from operator import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neuralNetwork (nn.Module):\n",
    "    \n",
    "    def __init__ (self):\n",
    "        \n",
    "        super(neuralNetwork, self).__init__()\n",
    "        \n",
    "        self._initial_nucleids = list(string.ascii_uppercase)[0:8]\n",
    "        self._generated_nucleids = list(string.ascii_uppercase)[8:]\n",
    "\n",
    "        self.networks = nn.ModuleDict()\n",
    "\n",
    "        for initial_nucleid in self._initial_nucleids:\n",
    "            self.networks[initial_nucleid] = nn.Sequential(*[\n",
    "                nn.Linear(13, 200),\n",
    "                nn.Dropout(0),\n",
    "                nn.BatchNorm1d(200),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(200, 100),\n",
    "                nn.Dropout(0),\n",
    "                nn.BatchNorm1d(100),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(100, 80),\n",
    "            ])\n",
    "\n",
    "        for generated_nucleid in self._generated_nucleids:\n",
    "            self.networks[generated_nucleid] = nn.Sequential(*[\n",
    "                nn.Linear(653, 500),\n",
    "                nn.Dropout(0),\n",
    "                nn.BatchNorm1d(500),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(500, 100),\n",
    "                nn.Dropout(0),\n",
    "                nn.BatchNorm1d(100),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(100, 80),\n",
    "            ])\n",
    "\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=5e-2)\n",
    "    \n",
    "    def forward(self, X):\n",
    "\n",
    "        # Separating dataset\n",
    "        y_hat = {}\n",
    "\n",
    "        # 1. Training the original data\n",
    "        for inital_nucleid in self._initial_nucleids:\n",
    "            y_hat[inital_nucleid] = self.networks[inital_nucleid](X)\n",
    "\n",
    "        # 2. Getting the prediction to predict the new nucleids\n",
    "        X_ = torch.concat(list(y_hat.values()), axis=1)\n",
    "        X_ = torch.concat([X, X_], axis=1)\n",
    "\n",
    "        for generated_nucleid in self._generated_nucleids:\n",
    "            y_hat[generated_nucleid] = self.networks[generated_nucleid](X_)\n",
    "\n",
    "        # 3. Generating full ouput\n",
    "        y_hat_final = torch.concat(list(y_hat.values()), axis=1)\n",
    "\n",
    "        return y_hat_final\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        self.train()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        y_hat = self.forward(X)\n",
    "        loss = self.loss_fn(y, y_hat)\n",
    "        loss.backward()\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.detach().item()\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            y_hat = self.forward(X)\n",
    "\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaling_ = np.max(X_train.values, axis=0, keepdims=True)\n",
    "Y_scaling_ = np.max(y_train_all.values, axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train.values/X_scaling_, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_all.values/Y_scaling_, dtype=torch.float32)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test.values/X_scaling_, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_all.values/Y_scaling_, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = neuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model.predict(X_train_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "\n",
    "for i in range(n_epochs):\n",
    "\n",
    "    if i%10 == 0 and i != 0:\n",
    "        y_hat_test = torch.exp(model.predict(torch.log(X_test_tensor)).detach()).numpy()*Y_scaling_\n",
    "        y_test_loss = mean_absolute_percentage_error(y_test_all.values, y_hat_test)\n",
    "\n",
    "        print(f\"Epoch {i} - MSE Loss {train_loss} - MAPE Test loss {y_test_loss}\")\n",
    "    \n",
    "    train_loss = model.fit(torch.log(X_train_tensor), torch.log(y_train_tensor))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "28b293e0c0671e44c7281dde6399c7c7419d3faca031d22494da8635907ada72"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
