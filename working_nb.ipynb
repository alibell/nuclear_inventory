{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import string\n",
    "\n",
    "def get_file_list_from_dir(*, path, datadir):\n",
    "    data_files = sorted(glob(os.path.join(path, \"data\", datadir, \"*.csv.gz\")))\n",
    "    return data_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = list(string.ascii_uppercase)  # to ease the manipulation of the data\n",
    "input_compos = alphabet[:8]\n",
    "input_params = input_compos + [\"p1\", \"p2\", \"p3\", \"p4\", \"p5\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dirty preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = get_file_list_from_dir(path=\".\", datadir=\"train\")\n",
    "dtrain = pd.concat((pd.read_csv(f) for f in train_files))\n",
    "\n",
    "train_data = dtrain[alphabet].add_prefix('Y_')\n",
    "train_data[\"times\"] = dtrain[\"times\"]\n",
    "train_data = train_data[ train_data[\"times\"] > 0.]\n",
    "temp = dtrain.loc[0][input_params].reset_index(drop=True)\n",
    "temp = temp.loc[temp.index.repeat(80)].reset_index(drop=True)\n",
    "train_data = pd.concat([temp, train_data.reset_index(drop=True)], axis=1)\n",
    "\n",
    "train_target_A = train_data.groupby(input_params)['Y_A'].apply(list).apply(pd.Series).rename(\n",
    "    columns=lambda x: 'A' + str(x + 1)).reset_index()\n",
    "train_target_A\n",
    "\n",
    "X_train = train_target_A[input_params]\n",
    "\n",
    "y_train_all = []\n",
    "for i in alphabet:\n",
    "    y_train_all.append(\n",
    "        train_data.groupby(input_params)['Y_'+i].apply(list).apply(pd.Series).rename(\n",
    "        columns=lambda x: i + str(x + 1)).reset_index().iloc[:, len(input_params):]\n",
    "    )\n",
    "    \n",
    "y_train_all = pd.concat(y_train_all, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files = get_file_list_from_dir(path=\".\", datadir=\"test\")\n",
    "dtest = pd.concat((pd.read_csv(f) for f in test_files))\n",
    "\n",
    "test_data = dtest[alphabet].add_prefix('Y_')\n",
    "test_data[\"times\"] = dtest[\"times\"]\n",
    "test_data = test_data[test_data[\"times\"] > 0.]\n",
    "temp = dtest.loc[0][input_params].reset_index(drop=True)\n",
    "temp = temp.loc[temp.index.repeat(80)].reset_index(drop=True)\n",
    "test_data = pd.concat([temp, test_data.reset_index(drop=True)], axis=1)\n",
    "\n",
    "test_target_A = test_data.groupby(input_params)['Y_A'].apply(list).apply(pd.Series).rename(\n",
    "    columns=lambda x: 'A' + str(x + 1)).reset_index()\n",
    "\n",
    "X_test = test_target_A[input_params]\n",
    "\n",
    "y_test_all = []\n",
    "for i in alphabet:\n",
    "    y_test_all.append(\n",
    "        test_data.groupby(input_params)['Y_'+i].apply(list).apply(pd.Series).rename(\n",
    "        columns=lambda x: i + str(x + 1)).reset_index().iloc[:, len(input_params):]\n",
    "    )\n",
    "    \n",
    "y_test_all = pd.concat(y_test_all, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Current work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from functools import reduce\n",
    "from operator import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neuralNetwork (nn.Module):\n",
    "    \n",
    "    def __init__ (self):\n",
    "        \n",
    "        super(neuralNetwork, self).__init__()\n",
    "        \n",
    "        self._initial_nucleids = list(string.ascii_uppercase)[0:8]\n",
    "        self._generated_nucleids = list(string.ascii_uppercase)[8:]\n",
    "\n",
    "        self.networks = nn.ModuleDict()\n",
    "\n",
    "        i = 0\n",
    "        input_features = 378\n",
    "\n",
    "        for initial_nucleid in self._initial_nucleids:\n",
    "            self.networks[initial_nucleid] = nn.Sequential(*[\n",
    "                nn.Linear(input_features+(i*80), 500),\n",
    "                nn.Dropout(0),\n",
    "                nn.BatchNorm1d(500),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(500, 150),\n",
    "                nn.Dropout(0),\n",
    "                nn.BatchNorm1d(150),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(150, 100),\n",
    "                nn.BatchNorm1d(100),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(100, 80),\n",
    "            ])\n",
    "            i += 1\n",
    "\n",
    "        for generated_nucleid in self._generated_nucleids:\n",
    "            self.networks[generated_nucleid] = nn.Sequential(*[\n",
    "                nn.Linear(input_features+(i*80), 500),\n",
    "                nn.Dropout(0),\n",
    "                nn.BatchNorm1d(500),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(500, 150),\n",
    "                nn.BatchNorm1d(150),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(150, 100),\n",
    "                nn.BatchNorm1d(100),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(100, 80),\n",
    "            ])\n",
    "            i += 1\n",
    "\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.optimizer = optim.AdamW(self.parameters(), lr=1e-2, amsgrad=True)\n",
    "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, patience=10, factor=0.5)\n",
    "    \n",
    "    def forward(self, X):\n",
    "\n",
    "        # Separating dataset\n",
    "        y_hat = {}\n",
    "\n",
    "        X_ = X\n",
    "\n",
    "        # 1. Training the original data\n",
    "        for inital_nucleid in self._initial_nucleids:\n",
    "            y_hat[inital_nucleid] = self.networks[inital_nucleid](X_)\n",
    "            X_ = torch.concat([X_, y_hat[inital_nucleid]], axis=1)\n",
    "\n",
    "        # 2. Getting the prediction to predict the new nucleids\n",
    "        for generated_nucleid in self._generated_nucleids:\n",
    "            y_hat[generated_nucleid] = self.networks[generated_nucleid](X_)\n",
    "            X_ = torch.concat([X_, y_hat[generated_nucleid]], axis=1)\n",
    "\n",
    "        # 3. Generating full ouput\n",
    "        y_hat_final = torch.concat(list(y_hat.values()), axis=1)\n",
    "\n",
    "        return y_hat_final\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        self.train()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        y_hat = self.forward(X)\n",
    "\n",
    "        loss = self.loss_fn(y, y_hat)\n",
    "        loss.backward()\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.detach().item()\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            y_hat = self.forward(X)\n",
    "\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing\n",
    "class pre_processor():\n",
    "    \n",
    "    def __init__ (self, degree=2):\n",
    "\n",
    "        self.polynomial = PolynomialFeatures(degree=degree)\n",
    "\n",
    "        # Liste des output Ã  exprimer en log\n",
    "        #self.to_log = [True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
    "        self.to_log = [True for x in list(string.ascii_uppercase)]\n",
    "        self.to_log = np.repeat(self.to_log, 80)\n",
    "\n",
    "    def fit(self, X,y):\n",
    "\n",
    "        X = check_array(X)\n",
    "        y = check_array(y, ensure_2d=False)\n",
    "\n",
    "        # Scaling in log\n",
    "        X_ = np.concatenate([np.log(X), X], axis=1)\n",
    "        y_ = np.copy(y)\n",
    "        y_[:, self.to_log] = np.log(y[:, self.to_log])\n",
    "\n",
    "        # Getting polynomial\n",
    "        X_ = self.polynomial.fit_transform(X_)\n",
    "\n",
    "        # Normalization\n",
    "        ## Computing parameters\n",
    "        self.X_mean = np.mean(X_, axis=0)\n",
    "        self.X_range = np.max(X_, axis=0)-np.min(X_, axis=0)+1e-8\n",
    "\n",
    "        self.y_mean = np.mean(y_, axis=0)\n",
    "        self.y_range = np.max(y_, axis=0)-np.min(y_, axis=0)+1e-8\n",
    "\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "\n",
    "        X = check_array(X)\n",
    "\n",
    "        # Getting polynomial\n",
    "        X_ = np.concatenate([np.log(X), X], axis=1)\n",
    "        X_ = self.polynomial.transform(X_)\n",
    "\n",
    "        if y is not None:\n",
    "            y = check_array(y, ensure_2d=False)\n",
    "\n",
    "            y_ = np.copy(y)\n",
    "            y_[:, self.to_log] = np.log(y[:, self.to_log])\n",
    "\n",
    "            y_ = (y_-self.y_mean)/self.y_range\n",
    "        else:\n",
    "            y_ = None\n",
    "\n",
    "        X_ = (X_-self.X_mean)/self.X_range\n",
    "\n",
    "        return X_, y_\n",
    "\n",
    "    def inverse_transform(self, y):\n",
    "\n",
    "        y = check_array(y, ensure_2d=False)\n",
    "        y_ = (y*self.y_range)+self.y_mean\n",
    "        y_[:, self.to_log] = np.exp(y_[:, self.to_log])\n",
    "\n",
    "        return y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "class basicDataset(Dataset):\n",
    "    def __init__ (self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__ (self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__ (self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = neuralNetwork()\n",
    "model = model.to(\"cuda:0\")\n",
    "\n",
    "preprocess = pre_processor()\n",
    "preprocess.fit(X_train, y_train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_preprocess, y_train_preprocess = preprocess.transform(X_train, y_train_all)\n",
    "X_test_preprocess, y_test_preprocess = preprocess.transform(X_test, y_test_all)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_preprocess, dtype=torch.float32)\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train_preprocess, dtype=torch.float32)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test_preprocess, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_preprocess, dtype=torch.float32)\n",
    "\n",
    "X_train_tensor = X_train_tensor.to(\"cuda:0\")\n",
    "y_train_tensor = y_train_tensor.to(\"cuda:0\")\n",
    "X_test_tensor = X_test_tensor.to(\"cuda:0\")\n",
    "\n",
    "train_dataset = basicDataset(X_train_tensor, y_train_tensor)\n",
    "\n",
    "# Using dataloader to benefit from stochasticity\n",
    "train_loader = DataLoader(train_dataset, batch_size=690, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 500\n",
    "\n",
    "for i in range(n_epochs):\n",
    "\n",
    "    if i%100 == 0 and i != 0:\n",
    "        y_hat_test = model.predict(X_test_tensor).detach().cpu().numpy()\n",
    "        y_hat_test_inverse_transform = preprocess.inverse_transform(y_hat_test)\n",
    "        y_test_loss = mean_absolute_percentage_error(y_test_all.values, y_hat_test_inverse_transform)\n",
    "\n",
    "        print(f\"Epoch {i} - MSE Loss {epoch_loss} - MAPE Test loss {y_test_loss}\")\n",
    "\n",
    "    losses = []\n",
    "    \n",
    "    for x,y in train_loader:\n",
    "        train_loss = model.fit(x, y)\n",
    "        losses.append(train_loss)\n",
    "\n",
    "    epoch_loss = np.sum(losses)\n",
    "    model.scheduler.step(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "28b293e0c0671e44c7281dde6399c7c7419d3faca031d22494da8635907ada72"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
